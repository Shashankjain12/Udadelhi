{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Federated Learning</center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federated Learning -  Collaborative Machine Learning Without Centralised Training Data\n",
    "\n",
    "First let's look what is standard machine learning Approach:-\n",
    "\n",
    "In standard ML approach there is a kind of centralised network where model is trained on just one single \n",
    "computer.\n",
    "\n",
    "To train a machine learning model, traditional machine learning adopts a centralized approach which requires the training data to be aggregated on a single machine or in a datacenter. This is practically what giant AI companies such as Google, Facebook, and Amazon have been doing over the years. These companies have been collecting a gigantic amount of data and store these data in their datacenters where machine learning models are trained. This centralized training approach, however, is privacy-intrusive, especially for mobile phone users. \n",
    "\n",
    "This is because mobile phones may contain the owners’ privacy-sensitive data. To train or obtain a better machine learning model under such a centralized training approach, mobile phone users have to trade their privacy by sending their personal data stored inside phones to the clouds owned by the AI companies.\n",
    "\n",
    "#### What is Federated Learning?\n",
    "\n",
    "Federated learning enables mobile phones to collaboratively learn a shared prediction model while keeping all the training data on device, decoupling the ability to do machine learning from the need to store the data in the cloud. This goes beyond the use of local models that make predictions on mobile devices (like the Mobile Vision API and On-Device Smart Reply) by bringing model training to the device as well.\n",
    "\n",
    "It works like this: your device downloads the current model, improves it by learning from data on your phone, and then summarizes the changes as a small focused update. Only this update to the model is sent to the cloud, using encrypted communication, where it is immediately averaged with other user updates to improve the shared model. All the training data remains on your device, and no individual updates are stored in the cloud.\n",
    "\n",
    "<img src=\"./images/Fedlearn_1.png\"></img>\n",
    "\n",
    "Federated Learning allows for smarter models, lower latency, and less power consumption, all while ensuring privacy. And this approach has another immediate benefit: in addition to providing an update to the shared model, the improved model on your phone can also be used immediately, powering experiences personalized by the way you use your phone.\n",
    "\n",
    "In Google search when Gboard shows a suggested query, your phone locally stores information about the current context and whether you clicked the suggestion. Federated Learning processes that history on-device to suggest improvements to the next iteration of Gboard’s query suggestion model.\n",
    "\n",
    "<img src=\"./images/gboard.gif\"></img>\n",
    "\n",
    "To make Federated Learning possible, we had to overcome many algorithmic and technical challenges. In a typical machine learning system, an optimization algorithm like Stochastic Gradient Descent (SGD) runs on a large dataset partitioned homogeneously across servers in the cloud. Such highly iterative algorithms require low-latency, high-throughput connections to the training data. But in the Federated Learning setting, the data is distributed across millions of devices in a highly uneven fashion. \n",
    "\n",
    "In addition, these devices have significantly higher-latency, lower-throughput connections and are only intermittently available for training.These bandwidth and latency limitations motivate our Federated Averaging algorithm, which can train deep networks using 10-100x less communication compared to a naively federated version of SGD. The key idea is to use the powerful processors in modern mobile devices to compute higher quality updates than simple gradient steps. Since it takes fewer iterations of high-quality updates to produce a good model, training can use much less communication. As upload speeds are typically much slower than download speeds, we also developed a novel way to reduce upload communication costs up to another 100x by compressing updates using random rotations and quantization. \n",
    "\n",
    "\n",
    "While these approaches are focused on training deep networks, we've also designed algorithms for high-dimensional sparse convex models which excel on problems like click-through-rate prediction.Deploying this technology to millions of heterogenous phones running Gboard requires a sophisticated technology stack. On device training uses a miniature version of TensorFlow. Careful scheduling ensures training happens only when the device is idle, plugged in, and on a free wireless connection, so there is no impact on the phone's performance.\n",
    "\n",
    "Federated learning works without the need to store user data in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges To Federated Learning \n",
    "\n",
    "Federated learning is confronted by two key challenges. One of the challenges is communication bandwidth. Federated learning on mobile phones relies on wireless communication to collaboratively learn a machine learning model. Although compute resources of mobile phones are becoming increasingly powerful, the bandwidth of wireless communication has not increased as much. As such, the bottleneck is shifted from computation to communication. As a consequence, limited communication bandwidth could incur long communication latency, and thus could significantly slow down the convergence time of the federated learning process.\n",
    "\n",
    "Another challenge that federated learning needs to address is the reliability of end devices which participate in the federated learning process. Federated learning is an iterative process, it relies on the participating end devices to continuously communicate over iterations until the learning process converges. However, in real-world deployments, due to various practical reasons, not all end devices may fully participate in the complete iterative process from beginning to end. For end devices which drop out in the middle of the federated learning process, their data cannot be fully utilized during the learning process. As such, the learning quality of federated learning could be considerably jeopardized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resourced from:-\n",
    "\n",
    "https://medium.com/syncedreview/federated-learning-the-future-of-distributed-machine-learning-eec95242d897\n",
    "\n",
    "https://ai.googleblog.com/2017/04/federated-learning-collaborative.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
